# -*- coding: utf-8 -*-
"""Final_SemanticBookRecommender 1 .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/139D48xMPtttxJ5j9LLX7BuIJBgbEfkVI
"""

# importing lib

import pandas as pd
import seaborn as sns
import numpy as np
import matplotlib.pyplot as plt
!pip install kagglehub
!pip install transformers
!pip install langchain
!pip install langchain_community
!pip install chromadb
!pip install gradio
from langchain_community.vectorstores import Chroma
from langchain.embeddings import SentenceTransformerEmbeddings
from google.colab import output  #ipy widgets
import kagglehub
import os
!pip install python-dotenv
from dotenv import load_dotenv

# dowloading dataset
# providing path thorugh kagglehub
# dataset used  : 7K books ( 6810 rows Ã— 12 columns)

path= kagglehub.dataset_download("dylanjcastillo/7k-books-with-metadata")
print("Path to my  dataset files:", path)

# we read using books.csv ,because this file present in dataset dir
books=pd.read_csv(f"{path}/books.csv")
books

# gives the full info of dataset

books.info()

# viewing first 5

books.head()

# viewing last five

books.tail()

# gives the statistical analysis of dataset

books.describe()

books.isnull().sum()

fig,axes=plt.subplots()
sns.heatmap(books.isnull().transpose(),cbar=False)
plt.xlabel('columns')
plt.ylabel('missing values')
plt.show()

# to find the missing description
# to find age of book using current year
# 1 = missing descriptiom
# 0 = presented decription

books['Missing description']=np.where(books['description'].isnull(),1,0)
books['age_of_book']=2025-books['published_year']

# taking account into desired columns
# corr matrix using 'spearman' , because of presence of binary val in missing description

columns_of_interest=['num_pages','age_of_book','Missing description','average_rating']
correlation_matrix=books[columns_of_interest].corr(method='spearman')
correlation_matrix
#published_year	average_rating	num_pages	ratings_count	Missing description	missing description	age

# visualising the corr_matrix
sns.set_theme(style='darkgrid')
plt.figure(figsize=(10,8))
heatmap=sns.heatmap(correlation_matrix,annot=True,fmt=".2f",cmap="coolwarm",cbar_kws={'label':'Spearman correlation reading'})
heatmap.set_title('correlation heatmap')
plt.title("Correlation Heatmap")
plt.show()

# to check missing values for column_of_interest

book_missing=books[~(books['description'].isna()) &
     ~(books['published_year'].isna()) &
     ~(books['average_rating'].isna()) &
     ~(books['num_pages'].isna())
]

book_missing

book_missing['categories'].value_counts().reset_index().sort_values("count",ascending=False)

book_missing

book_missing["words_in_description"]=book_missing["description"].str.split().str.len()

book_missing

book_missing.loc[book_missing["words_in_description"].between(25,34),"description"]

book_missing_25_words = book_missing[book_missing["words_in_description"] >= 25 ]
book_missing_25_words

book_missing_25_words["title_and_subtitle"]=(
    np.where(book_missing_25_words["subtitle"].isna(),book_missing_25_words["title"],
             book_missing_25_words[["title","subtitle"]].astype(str).agg(":".join,axis=1)))

book_missing_25_words

book_missing_25_words['tagged_description']=book_missing_25_words[['isbn13','description']].astype(str).agg(" ".join,axis=1)

book_missing_25_words

# saving the changes to books_cleaned variable

(
  book_missing_25_words.drop(['subtitle','words_in_description','age_of_book','Missing description'],axis=1).to_csv("books_cleaned.csv",index=False)
)

# loading data
#splitting documents into chunks
# intialize embedddings
# building vector db using chromadb

from langchain_community.document_loaders  import TextLoader # to convert the text into the form to support langchain
from langchain_text_splitters import CharacterTextSplitter # for splitting text into chunks
from sentence_transformers import SentenceTransformer # for embeddinging from BGE embedding
from langchain_community.vectorstores import Chroma  # for vector db

# assigning the changes to books
books=pd.read_csv("books_cleaned.csv")
books

books['tagged_description']

# we cannot use df in ,.textloader of langchain
# so , i'll create a text file and i will save the tagged description of books df to it
# connecting my drive acc to create txt file in colab

books["tagged_description"].to_csv("tagged_description.txt",
                                 sep='\n',
                                 index=False,
                                 header=False)

# loading the tagged_description.txt to the textloader

raw_Documents=TextLoader('/content/tagged_description.txt').load()
text_splitter=CharacterTextSplitter(chunk_size=0,chunk_overlap=0,separator='\n')
Documents=text_splitter.split_documents(raw_Documents)

# viewing , to confirm the spliitng porcess done in chunks
Documents[:10]

# using BGE embedding instead of openai apikey which is subsricption free

from sentence_transformers import SentenceTransformer
from langchain.embeddings import SentenceTransformerEmbeddings
embeddings=SentenceTransformerEmbeddings(model_name="BAAI/bge-large-en")

# HuggingFaceEmbeddings object in LangChain
# using the ' BAAI/bge-large-en ' SentenceTransformer model.
# info about embedding  used
# 512 tokens for input search

# embeddings
# viewing the embedding
# embed.query = perfoms the embeddings to store in vectordb

text_to_embed = Documents[0].page_content
embedded_text = embeddings.embed_query(text_to_embed)
embedded_text

# building vector database
# We use persist dir because the db will be saved ,
# even when we close the colab or restart the runtime ,used for effiecient retreival
# db is not lost, and we can share the db ,by providing the following directory

Vector_db=Chroma.from_documents(
    documents=Documents,
    embedding=embeddings,
    persist_directory="BOOKS_VDB"
)

Vector_db.persist()

# viewing the content in  vector db
# importiung sqlite3

import sqlite3
connection=sqlite3.connect('BOOKS_VDB/chroma.sqlite3')
cursor=connection.cursor()

cursor.execute("select * from embeddings")
results=cursor.fetchall()
results

# querying to check recommendation
# similarity_search works

query=" a book about world war 1"
docs=Vector_db.similarity_search(query,k=10)
docs

# we should provide recommendation with book name and isbn no ,not the description
# working with isbn

books[books['isbn13']==int(docs[0].page_content.split()[0].strip())]

# creating func to retrieve semantic recommendation

def Retrieve_Semantic_Recommendations(
    User_Query : str,
    top_k : int=10,

):
    import pandas as pd
    Recommend=Vector_db.similarity_search(User_Query,k=50)

    # empty list store the content of 'isbn13 column'

    Books_List=[]

    for i in range(0,len(Recommend)):
      Books_List += [int(Recommend[i].page_content.strip('"').split()[0])]
    return books[books["isbn13"].isin(Books_List)].head(top_k)

Retrieve_Semantic_Recommendations("book to teach children about nature")

# checking the count of categories
books['categories'].value_counts().reset_index()

# checking in random ,categories that have books more than 50
books['categories'].value_counts().reset_index().query("count>50")

# assigning the categories for all the categories present , as fiction and non-fiction through mapping

category_mapping={
    'Fiction' :'Fiction',
    'Juvenile fiction':'Childrens Fiction',
    'Biography & Autobiography':'Nonfiction',
    'History':'Nonfiction',
    'Literacy Criticism':'Nonfiction',
    'Philosophy':'Nonfiction',
    'Religion':'Nonfiction',
    'Comics & Graphics Novels':'fiction',
    'Drama':'fiction',
    'Juvenile  Nonfiction':'Childrens Nonfiction',
    'Science':'Nonfiction',
    'Poetry':'fiction'}
books['simple_categories']=books['categories'].map(category_mapping)

# created new column simple categories to map as fiction and non-fiction
books

# selecting simple categories where it is not a 'NaN'
books[~books['simple_categories'].isna()]

# transfoemr architecture
# /content/Transfoemer.architecture.png
# fetching LLM on hugging face for zero-shot classification
# model = "facebook/bart-large-mnli"

from transformers import pipeline
fiction_categories=['Fiction','Nonfiction']
zeroshot_pipe= pipeline("zero-shot-classification",
                model="facebook/bart-large-mnli",
                device=0)

sequence=books.loc[books['simple_categories']=='Fiction','description'].reset_index(drop=True)[0]
sequence

zeroshot_pipe(sequence,fiction_categories)

# zeroshot llm
# 'labels': ['Fiction', 'Nonfiction'],
# 'scores': [0.8438267111778259, 0.15617327392101288]

# our model correctly predicted the given categories as fiction
# by giving more percentage for fiction than nonfiction

#uses of np.argmax and the purpose of using it
# even after predicting the percentage of fiction and non fiction

# Generalizing the category selection process for scenarios with more than two categories.
# Enabling programmatic selection of the predicted category based on the highest score.
# Maintaining consistency with typical classification workflows.

import numpy as np
max_index=np.argmax(zeroshot_pipe(sequence,fiction_categories)['scores'])
max_label=zeroshot_pipe(sequence,fiction_categories)['labels'][max_index]
max_label

# so 0 = fiction , 1 = non-fiction

def generate_prediction(sequence,categories):
  predictions=zeroshot_pipe(sequence,categories)
  max_index=np.argmax(predictions['scores'])
  max_label=predictions['labels'][max_index]
  return  max_label

max_label

# taking first 300 samples and testing with known fiction and Nonfiction
# and appending to the fiction_categories belong in generate_prediction func

from tqdm import tqdm
actual_cats=[]
predicted_cats=[]

# for Fiction
for i in tqdm(range(0,300)):
  print(f'{i}/300')
  sequence=books.loc[books['simple_categories']=='Fiction','description'].reset_index(drop=True)[i]
  predicted_cats += [generate_prediction(sequence,fiction_categories)]
  actual_cats += ['Fiction']

# for non-Fiction
for i in tqdm(range(0,300)):
  print(f'{i}/300')
  sequence=books.loc[books['simple_categories']=='Nonfiction','description'].reset_index(drop=True)[i]
  predicted_cats += [generate_prediction(sequence,fiction_categories)]
  actual_cats += ['Nonfiction']

# 0%| | 0/500 [00:00<?, ?it/s]:

# 0%: Indicates the percentage of the task completed (0% at the beginning).

# | |: The progress bar itself. The empty space represents the remaining work.

# 0/500: Shows the current iteration (0) out of the total iterations (500).

# [00:00<?, ?it/s]:

# 00:00: Elapsed time since the start of the task.

# ?: Estimated time remaining (unknown at the beginning).

# ?it/s: Estimated iterations per second (unknown at the beginning).

# 0%| | 2/500 [00:00<00:43, 11.33it/s]:

# Similar to the first line, but now the progress bar has advanced to 2 out of 500 iterations.

# 00:43: Estimated remaining time to completion (43 seconds).

# 11.33it/s: Estimated iterations per second (11.33 iterations per second).

# creating df predictions_df , to compare the predictionsa made

predictions_df=pd.DataFrame({'actual_categories':actual_cats,'predicted_categories':predicted_cats})
predictions_df

predictions_df['correct_predictions']=(
    np.where(predictions_df['actual_categories']==predictions_df['predicted_categories'],1,0)
)

predictions_df['correct_predictions'].sum()/len(predictions_df)

# Accuracy:0.801666 =  for the descriptions that were present

# finding the missing categories using the book number isbns
# books = dataframe

isbns=[]
predicted_cats =[]

missing_cats=books.loc[books['simple_categories'].isna(),['isbn13','description']].reset_index(drop=True)

for i in tqdm(range(0,len(missing_cats))):
  sequence=missing_cats['description'][i]
  predicted_cats += [generate_prediction(sequence,fiction_categories)]
  isbns += [missing_cats['isbn13'][i]]

missing_predicted_df=pd.DataFrame({'isbn13':isbns,'predicted_categories':predicted_cats})
missing_predicted_df

books=pd.merge(books,missing_predicted_df,on='isbn13',how='left')
books['simple_categories']=np.where(books['simple_categories'].isna(),books['predicted_categories'],books['simple_categories'])
books=books.drop(columns=['predicted_categories'])

# we have predicted the categories and it workes well and it matches the actual  and injected into the books
# so, having the actual catgories and we can remove the predicted categories

books

books[books['categories'].str.lower().isin([
    "romance",
    "science fiction",
    "scifi",
    "fantasy",
    "horror",
    "mystery",
    "thriller",
    "crime",
    "history fiction",
    "thriller",
    "comedy",
    "historical"
])]

books.to_csv("book_with_catgories.csv",index=False)

import pandas as pd
books=pd.read_csv("book_with_catgories.csv")

# llm for emotion extraction from text

from transformers import pipeline
emotion_llm= pipeline("text-classification",
                    model="j-hartmann/emotion-english-distilroberta-base",
                    top_k=None,
                    device=0)
emotion_llm("I love this!")

# checking emotion of our description
books['description'][0]

# checking emotion of our description
emotion_llm(books['description'][0])

emotion_llm(books['description'][0].split("."))

sentences=books['description'][0].split(".")
predictions_emotion=emotion_llm(sentences)

sentences[0]

predictions_emotion[0]

sentences[5]

predictions_emotion[5]

# arranging the score by labels

sorted(predictions_emotion[0],key=lambda x:x['label'])

sorted(predictions_emotion[0],key=lambda x:x['score'])

# func to extract the emotions of the description of book

emotion_labels=["anger","disgust","fear","joy","neutral","sadness","surprise"]
isbn=[]
emotion_scores={label: [] for label in emotion_labels}

def calculate_max_emotion_score(predictions_emotion):
    per_emotion_scores ={label :[] for label in emotion_labels}
    for emot_predict in predictions_emotion:
        sorted_predictions=sorted(emot_predict,key=lambda x:x ['label'])
        for index,label in enumerate(emotion_labels):
          per_emotion_scores[label].append(sorted_predictions[index]["scores"])
    return {label:np.max(scores) for label,scores in per_emotion_scores.items()}

from tqdm import tqdm
import pandas as pd

books = pd.read_csv("book_with_catgories.csv")

emotion_labels=["anger","disgust","fear","joy","neutral","sadness","surprise"]
isbn=[]
emotion_scores={label: [] for label in emotion_labels}


def calculate_max_emotion_score(predictions_emotion):
    per_emotion_scores ={label :[] for label in emotion_labels}
    for emot_predict in predictions_emotion:
        sorted_predictions=sorted(emot_predict,key=lambda x:x ['label'])
        for index,label in enumerate(emotion_labels):
          per_emotion_scores[label].append(sorted_predictions[index]["score"])
    return {label:np.max(scores) for label,scores in per_emotion_scores.items()}

for i in tqdm(range(len(books))):
  isbn.append(books['isbn13'][i])
  sentences=books['description'][i].split(".")
  predictions=emotion_llm(sentences)
  max_scores=calculate_max_emotion_score(predictions)
  for label in emotion_labels:
    emotion_scores[label].append(max_scores[label])

emotions_df=pd.DataFrame(emotion_scores)
emotions_df['isbn13']=isbn

emotions_df.head() # emotion analysis for first 5 records

# merging to main df books

books=pd.merge(books,emotions_df,on='isbn13')

books

# saving to books withh emotions
books.to_csv("books_emotions.csv",index=False)

books

# user interface (gradio)

import gradio as gr
import pandas as pd
import numpy as np
from langchain.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.embeddings import SentenceTransformerEmbeddings
from sentence_transformers import SentenceTransformer
from langchain.embeddings import SentenceTransformerEmbeddings

embeddings=SentenceTransformerEmbeddings(model_name="BAAI/bge-large-en")


books=pd.read_csv("books_emotions.csv")

books['large_thumbnail']=books['thumbnail'] +"&fife=w800"
books['large_thumbnail']=np.where(books['large_thumbnail'].isna(),"/content/page-found-concept-illustration_114360-1869.jpg",
                                  books['large_thumbnail'],
)




raw_Documents=TextLoader('/content/tagged_description.txt').load()
text_splitter=CharacterTextSplitter(chunk_size=0,chunk_overlap=0,separator='\n')
Documents=text_splitter.split_documents(raw_Documents)
Vector_db=Chroma.from_documents(documents=Documents,embedding=embeddings,persist_directory="BOOKS_VDB")

def Retrieve_Semantic_Recommendations(
    query:str,
    category:str=None,
    tone:str = None,
    initial_top_k:int=50,
    final_top_k:int=16) ->pd.DataFrame:


    Recommends=Vector_db.similarity_search_with_score(query,k=initial_top_k)
    books_list=[int(Recommend.page_content.strip('"').split()[0]) for Recommend in Recommends]
    books_Recommends=books[books['isbn13'].isin(books_list)].head(final_top_k)

    if category !='all':
      books_Recommends=books_Recommends[books_Recommends['simple_categories']==category].head(final_top_k)
    else:
      books_Recommends=books_Recommends.head(final_top_k)

    if tone=="happy":
      books_Recommends.sort_values(by='joy',ascending=False,inplace=True)
    elif tone=="sad":
      books_Recommends.sort_values(by='sadness',ascending=False,inplace=True)
    elif tone=="angry":
      books_Recommends.sort_values(by='anger',ascending=False,inplace=True)
    elif tone=="neutral":
      books_Recommends.sort_values(by='neutral',ascending=False,inplace=True)
    elif tone=="suspenseful":
      books_Recommends.sort_values(by='fear',ascending=False,inplace=True)
    elif tone=="surprising":
      books_Recommends.sort_values(by='surprise',ascending=False,inplace=True)

    return books_Recommends.head(final_top_k)


def recommend_books(
    query:str,
    category:str,
    tone:str
):


    recommendation=Retrieve_Semantic_Recommendations(query,category,tone)
    results=[]

    for _, row in recommendation.iterrows():
      description=row['description']
      truncated_desc_split=description.split()[:30]
      truncated_description=" ".join(truncated_description.split[:30]) + "..."



      authors_split=row["authors"].split(":")
      if len(authors_split)==2:
        authors_str=f"{authors_split[0]} and  {authors_split[1]}"
      elif len(authors_split)>2:
        authors_str=f"{', '.join(authors_split[:-1])}, and  {authors_split[-1]}"
      else:
        authors_str=row['authors']


      caption =f"{row['title']} by  {authors_str}: {truncated_description}"
      results.append(row['large_thumbnail'],caption)

    return results

categories=['All'] + sorted(books['simple_categories'].unique())
tones=['All'] + ["happy","sad","angry","surprising","neutral","suspenseful"]

with gr.Blocks(theme=gr.themes.Glass())  as dashboard:
  gr.Markdown("# Semantic Book Recommender")

  with gr.Row():
    user_query=gr.Textbox(label="enter a description of a book:",
                          placeholder="e.g.., A story about nature")
    category_dropdown=gr.Dropdown(choices=categories,label="Select category:",value ='All')
    tone_dropdown=gr.Dropdown(choices=tones,label='Select emotional tone:',value ='All')
    submit_button=gr.Button("Find Recommendation")

  gr.Markdown("## Recommendations")
  output=gr.Gallery(label='Recommended Books',columns= 8,rows=2)


  submit_button.click(fn=recommend_books,
                      inputs=[user_query,category_dropdown,tone_dropdown],
                      outputs=output)

if __name__=='__main__':
  dashboard.launch()

